{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from mlxtend.evaluate import (\n",
    "    cochrans_q,\n",
    "    mcnemar,\n",
    "    mcnemar_table\n",
    ")\n",
    "from itertools import combinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01: Significance Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Types of significance tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare different classification models, which cannot be done with correlation tests, like Pearsons's Correlation.\n",
    "\n",
    "\n",
    "List of possible tests that could be applied to data which holds predicted and true classification values:\n",
    "* McNemar: only for two parameter to compare\n",
    "* Cochran's Q\n",
    "* 5x2 CV Paired t\n",
    "\n",
    "Since we have more than two parameters, we either need to check pair-wise with McNemar or we can use the Cochran Q test. We opted to first see the results of our hypothesis with a Cochran Q test and have a deeper investigation with pair-wise McNemar tests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_{0}$: There is no difference between the classification accuracy.\n",
    "\n",
    "$H_{A}$: There are difference between the classification accuracy.\n",
    "\n",
    "Significance threshold: $\\alpha = 0.05$\n",
    "\n",
    "This threshold is used to determine if the null hypothesis holds or can be rejected when compared to the p-value:\n",
    "* $p > \\alpha$ &rarr; $H_{0}$ cannot be rejected\n",
    "* $p < \\alpha$ &rarr; $H_{0}$ can be rejected which results in $H_{A}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Apply test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForestClassifier()</th>\n",
       "      <th>LinearSVC()</th>\n",
       "      <th>KNeighborsClassifier()</th>\n",
       "      <th>MLPClassifier()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truth</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictions</th>\n",
       "      <td>[0, 1, 8, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "      <td>[0, 1, 8, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      RandomForestClassifier()  \\\n",
       "truth        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...   \n",
       "predictions  [0, 1, 8, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...   \n",
       "\n",
       "                                                   LinearSVC()  \\\n",
       "truth        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...   \n",
       "predictions  [0, 1, 2, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...   \n",
       "\n",
       "                                        KNeighborsClassifier()  \\\n",
       "truth        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...   \n",
       "predictions  [0, 1, 8, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...   \n",
       "\n",
       "                                               MLPClassifier()  \n",
       "truth        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...  \n",
       "predictions  [0, 1, 2, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_json('../data/digits_classifiers.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth: these values are the same for every classification model\n",
    "truth = df['RandomForestClassifier()'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q (or chi-squared): 107.33 \n",
      "p-value: 4.1168104053085647e-23\n"
     ]
    }
   ],
   "source": [
    "# calculate conchans q for every classification model with truth values\n",
    "q, p_value =  cochrans_q(\n",
    "    np.array(truth),\n",
    "    np.array(df.iloc[1, 0]),\n",
    "    np.array(df.iloc[1, 1]),\n",
    "    np.array(df.iloc[1, 2]),\n",
    "    np.array(df.iloc[1, 3])\n",
    ")\n",
    "\n",
    "print(f\"Q (or chi-squared): {q:.2f} \\np-value: {p_value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is smaller than $\\alpha$ ($4.11 \\times 10^{-23} < 0.05$), we can reject the null hypothesis and conclude that the classification accuracy differs between the models.\n",
    "\n",
    "To further investigate, we can now apply pair-wise test to every model with `McNemar`. With this approach we can determine if we can find \"a clear winner\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible combinations labels: 6\n",
      "Possible combinations values: 6\n"
     ]
    }
   ],
   "source": [
    "# get all combinations for McNemar approach\n",
    "list_labels = list(combinations(df.columns, 2))\n",
    "list_values = list(combinations(df.iloc[1], 2))\n",
    "\n",
    "print(f\"Possible combinations labels: {len(list_labels)}\")\n",
    "print(f\"Possible combinations values: {len(list_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_table_and_score(truth_values, combination_values, combination_labels):\n",
    "    print(f\"Comparison of {combination_labels[0]} and {combination_labels[1]}\\n\")\n",
    "\n",
    "    tb = mcnemar_table(\n",
    "        np.array(truth),\n",
    "        np.array(combination_values[0]),\n",
    "        np.array(combination_values[1])\n",
    "    )\n",
    "\n",
    "    print(f\"McNemar table:\\n{tb}\\n\")\n",
    "\n",
    "    chi2, p = mcnemar(tb, corrected=True)\n",
    "\n",
    "    print(f\"chi-squared: {chi2:.2f} \\np-value: {p}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate significant statisticall difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of RandomForestClassifier() and LinearSVC()\n",
      "\n",
      "McNemar table:\n",
      "[[1578  111]\n",
      " [  40   68]]\n",
      "\n",
      "chi-squared: 32.45 \n",
      "p-value: 1.2227811884233145e-08\n",
      "----------\n",
      "Comparison of RandomForestClassifier() and KNeighborsClassifier()\n",
      "\n",
      "McNemar table:\n",
      "[[1667   22]\n",
      " [  63   45]]\n",
      "\n",
      "chi-squared: 18.82 \n",
      "p-value: 1.4338726289171642e-05\n",
      "----------\n",
      "Comparison of RandomForestClassifier() and MLPClassifier()\n",
      "\n",
      "McNemar table:\n",
      "[[1637   52]\n",
      " [  42   66]]\n",
      "\n",
      "chi-squared: 0.86 \n",
      "p-value: 0.3532628012845984\n",
      "----------\n",
      "Comparison of LinearSVC() and KNeighborsClassifier()\n",
      "\n",
      "McNemar table:\n",
      "[[1594   24]\n",
      " [ 136   43]]\n",
      "\n",
      "chi-squared: 77.01 \n",
      "p-value: 1.7041780183157275e-18\n",
      "----------\n",
      "Comparison of LinearSVC() and MLPClassifier()\n",
      "\n",
      "McNemar table:\n",
      "[[1585   33]\n",
      " [  94   85]]\n",
      "\n",
      "chi-squared: 28.35 \n",
      "p-value: 1.014322925494309e-07\n",
      "----------\n",
      "Comparison of KNeighborsClassifier() and MLPClassifier()\n",
      "\n",
      "McNemar table:\n",
      "[[1654   76]\n",
      " [  25   42]]\n",
      "\n",
      "chi-squared: 24.75 \n",
      "p-value: 6.518503507391278e-07\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# loop over every combination\n",
    "for i in range(len(list_labels)):\n",
    "    mcnemar_table_and_score(truth, list_values[i], list_labels[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With results from the McNemar analysis we now can evaluate if there is a clear winner in terms of accuracy of the prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify best model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestClassifier() and LinearSVC()**\n",
    "\n",
    "The p-value is smaller than $\\alpha$ and therefore we can reject our null hypothesis. Looking at the 2x2 matrix, we see, that `RandomForestClassifier` predicted 111 correct, which `LinearSVC` predicted wrong. Compared to the vise versa 40 correctly predicted values yield to a 111:40 ratio in the favor of `RandomForestClassifier`.\n",
    "\n",
    "This concludes: $RandomForestClassifier > LinearSVC$\n",
    "\n",
    "**RandomForestClassifier() and KNeighborsClassifier()**\n",
    "\n",
    "The p-value is smaller than $\\alpha$ and therefore we an reject our null hyphothesis. The 2x2 matrix indicates, that `KNeighborsClassifier` successfully predicted 63 values, which `RandomForestClassifier` could not. Again, yielding to a ratio of 63:22 in favor of `KNeighborsClassifier`.\n",
    "\n",
    "This concludes: $KNeighborsClassifier > RandomForestClassifier > LinearSVC$\n",
    "\n",
    "**RandomForestClassifier() and MLPClassifier()**\n",
    "\n",
    "Have a p-value of 0.35, which is bigger than our $\\alpha$ value and therefore we cannot reject our null hypothesis and conclute that these two models perform equally well.\n",
    "\n",
    "This concludes: $KNeighborsClassifier > RandomForestClassifier = MLPClassifier > LinearSVC$\n",
    "\n",
    "**LinearSVC() and KNeighborsClassifier()**\n",
    "\n",
    "Is already conducted from the observations from above, but p-value is smaller than $\\alpha$ which allows us to reject our null hypothesis once again.\n",
    "\n",
    "The results stays the same: $KNeighborsClassifier > RandomForestClassifier = MLPClassifier > LinearSVC$\n",
    "\n",
    "**LinearSVC() and MLPClassifier()**\n",
    "\n",
    "p-value is smaller than $\\alpha$ and this leads to the same result as already derived from above: $KNeighborsClassifier > RandomForestClassifier = MLPClassifier > LinearSVC$\n",
    "\n",
    "**KNeighborsClassifier() and MLPClassifier()**\n",
    "\n",
    "p-value is also smaller than $\\alpha$ in this comparison, which also undermines the result we already have: $KNeighborsClassifier > RandomForestClassifier = MLPClassifier > LinearSVC$\n",
    "\n",
    "So, we can say `KNeighborsClassifier` would be the winner because in comparison to other models, it had significant statistically difference (p-values smaller than $\\alpha$) and had the best ratios for correctly predictes values where others predicted wrong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (main, Sep 14 2022, 22:38:23) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
